{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is for pruning the original dataset \n",
    "# https://www.kaggle.com/smid80/coronavirus-covid19-tweets-late-april\n",
    "# each file is about 250mb, adding up to around 2.03GB\n",
    "# this file removes non-english tweets\n",
    "# removes users with less than 500 followers\n",
    "# removes tweets with less than 10 likes\n",
    "# removes tweets with less than 10 retweets\n",
    "# removes unnecessary columns: \"created_at\", \"user_id\", \"reply_to_user_id\", \"is_quote\", \"place_full_name\", \"place_type\", \"account_lang\", \"account_created_at\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "\n",
    "hashtag_re = re.compile(\"(?:^|\\s)[＃#]{1}(\\w+)\", re.UNICODE)\n",
    "mention_re = re.compile(\"(?:^|\\s)[＠ @]{1}([^\\s#<>[\\]|{}]+)\", re.UNICODE)"
   ]
  },
  {
   "source": [
    "# this file is primarily to clean up the datasets that we have\n",
    "\n",
    "path = \"2020-*.csv\"\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for fname in glob.glob(path):\n",
    "    datasets.append(fname)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['2020-03-29 Coronavirus Tweets.CSV',\n",
       " '2020-03-30 Coronavirus Tweets.CSV',\n",
       " '2020-03-31 Coronavirus Tweets.CSV',\n",
       " '2020-04-01 Coronavirus Tweets.CSV',\n",
       " '2020-04-02 Coronavirus Tweets.CSV',\n",
       " '2020-04-03 Coronavirus Tweets.CSV',\n",
       " '2020-04-04 Coronavirus Tweets.CSV',\n",
       " '2020-04-05 Coronavirus Tweets.CSV',\n",
       " '2020-04-06 Coronavirus Tweets.CSV',\n",
       " '2020-04-07 Coronavirus Tweets.CSV',\n",
       " '2020-04-08 Coronavirus Tweets.CSV',\n",
       " '2020-04-09 Coronavirus Tweets.CSV',\n",
       " '2020-04-10 Coronavirus Tweets.CSV',\n",
       " '2020-04-11 Coronavirus Tweets.CSV',\n",
       " '2020-04-12 Coronavirus Tweets.CSV',\n",
       " '2020-04-13 Coronavirus Tweets.CSV',\n",
       " '2020-04-14 Coronavirus Tweets.CSV',\n",
       " '2020-04-15 Coronavirus Tweets.CSV',\n",
       " '2020-04-16 Coronavirus Tweets.CSV',\n",
       " '2020-04-17 Coronavirus Tweets.CSV',\n",
       " '2020-04-18 Coronavirus Tweets.CSV',\n",
       " '2020-04-19 Coronavirus Tweets.CSV',\n",
       " '2020-04-20 Coronavirus Tweets.CSV',\n",
       " '2020-04-21 Coronavirus Tweets.CSV',\n",
       " '2020-04-22 Coronavirus Tweets.CSV',\n",
       " '2020-04-23 Coronavirus Tweets.CSV',\n",
       " '2020-04-24 Coronavirus Tweets.CSV',\n",
       " '2020-04-25 Coronavirus Tweets.CSV',\n",
       " '2020-04-26 Coronavirus Tweets.CSV',\n",
       " '2020-04-27 Coronavirus Tweets.CSV',\n",
       " '2020-04-28 Coronavirus Tweets.CSV',\n",
       " '2020-04-29 Coronavirus Tweets.CSV',\n",
       " '2020-04-30 Coronavirus Tweets.CSV']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "allDfs = []\n",
    "\n",
    "for data in datasets:\n",
    "    df = pd.read_csv(data)\n",
    "\n",
    "    df = df.drop([\"user_id\", \"reply_to_user_id\", \"place_full_name\", \"place_type\", \"account_lang\", \"account_created_at\"], axis=1)\n",
    "\n",
    "    # df = df[(df['lang'] == \"en\")]\n",
    "    df = df[(df['favourites_count'] >= 10)]\n",
    "    df = df[(df['retweet_count'] >= 10)]\n",
    "    \n",
    "    df[\"hashtags\"] = df[\"text\"].apply(lambda x: [re.sub(r'\\W+', '', hasht).lower() for hasht in hashtag_re.findall(x)])\n",
    "    df[\"mentions\"] = df[\"text\"].apply(lambda x: re.findall(r'\\B@\\w*[a-zA-Z]+\\w*', x))\n",
    "\n",
    "    allDfs.append(df)\n",
    "\n",
    "frame = pd.concat(allDfs, axis=0, ignore_index=True)\n",
    "frame.to_csv(\"merged-dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}